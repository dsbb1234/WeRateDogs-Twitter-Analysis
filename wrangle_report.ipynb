{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"### WeRateDogs Twitter Data Wrangling Report\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The purpose of this study is to ingest Twitter data from a variety of sources and formats, assess it for quality and tidyness, clean and analyze the data.  Since the collection of this data was not through a random experiment,  findings are NOT assumed to have causality.\n",
    "\n",
    "Data for this project came from the archive of  twitter user @dog_rates archive known as WeRateDogs.  Users send the account pictures of their dogs and they receive a witty response and a rating of the dog.  The rating ranges from 0 to 13 (denominator) and is compared to 10 as the numerator.  WeRateDogs does not follow rigid rating.  They have values above \n",
    "\n",
    "Twitter provided Udacity permission to use the archive and Twitter also granted this author \"Student\" access to Tweety (their API) to download the tweet data associated with this project.\n",
    "\n",
    "Training for techiques used in this analysis came from Udacity Data Analysis class and from pandas and matplotlib documentation at the following URLs:\n",
    "\n",
    "https://classroom.udacity.com/nanodegrees/nd002/parts/af503f34-9646-4795-a916-190ebc82cb4a\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extract.html?highlight=str%20extract#pandas.Series.str.extract\n",
    "\n",
    "https://matplotlib.org/3.1.0/gallery/index.html\n",
    "\n",
    "https://stackoverflow.com/search?q=save+output+as+.png+matplotlib\n",
    " \n",
    "### Data Ingestion\n",
    "\n",
    "After importing pandas, numpy, and requests, created the code to import twitter-arhive-enhanced. \n",
    "\n",
    "Applied to Twitter and obtained a student developer account to access the Tweepy API.  Utilized Tweety API code supplied by Udacity to query the Tweety API for the tweet ids in the twitter-archive-enhanced csv file.  The code created a json file called tweet_json.txt.  Found errors in the download for certain tweet ids.  Determined they were due primarily due to the timer\n",
    "\n",
    "Googled StackExchange to figure out how to create borders around output. Added borders to the output to improve visibility.  The code/technique to do this was obtained on Stack Exchange: https://stackoverflow.com/questions/48798508/jupyter-pandas-dataframe-output-table-format-configuration\n",
    "\n",
    "Read in the json file and called it tweets.  Displayed the contents of that file.  Read the image predictions file from the internet using requests.get and looked at the the file contents.\n",
    "\n",
    "The data wrangling started by understanding what data is contained in each of the files.  Started with twitter_archive_enhanced.  Looking at file contents, dug into each variable.  Started with the \"names\" variable. At first glance, we saw there are 745 values of \"None\". In addition, there are some non-name words in the file like \"a\", \"the\", and \"an\".  Looking through the text variable, some names do appear.  Need to fill in any missing names available in the text field. In addition, there are some non-name words in the file like \"a\", \"the\", and \"an\".\n",
    "\n",
    "Set the maximum rows displayed to 1000 in order to see all of the names in order to make a list of names to delete.\n",
    " \n",
    "Next, looked at retweeted_status_id to determine if it can be used to identify retweeted rows.  The retweeted_status_id should provide a guide to removal of the records.\n",
    "\n",
    "Furthering the understanding of the variables, we will look at the doggo, puppo, pupper, and floofer fields to see how it is populated using value_counts(). We can see that doggo has the value of doggo if the animal is identified in that stage, otherwise it is populated with None. This is a tidy data issue. doggo, puppo, pupper, and floofer should be moved under a single variable, called development_stage. This eliminates additional variable\n",
    "\n",
    "We now turn to the rating numerator and denominator using value_counts().  Looking at the rating numerator, we see values as high as 1776. The WeRateDogs Wikipedia page indicated the ratings numerator is is up to 13, however, the rating structure is not followed rigidly and is considered one of the things appealing to followers. In this study, the high rating records were not removed and the analysis will not utilize the rating information.  Looking at the rating numerator, we see values as high as 1776. The WeRateDogs Wikipedia page indicated the ratings numerator is is up to 13, however, the rating structure is not followed rigidly and is considered one of the things appealing to followers. In this study, the high rating records were not removed and the analysis will not utilize the rating information.  Looking at the rating denominator, we see values ranging from 170 to 0. As the ratings are quirky and will not be used in the study, the records in excess of 13 will not be removed.\n",
    "\n",
    "Then looked at min and max values for fields in the twitter_archive_enhanced dataset.  Looking at the min values of the twitter_archive_enhanced dataset, we see that the text contains the Twitter URL. This should be put in a separate column (Tidyness issue). Looking at the min values of the twitter_archive_enhanced dataset, we see that the text contains the Twitter URL. This should be put in a separate column (Tidyness issue).\n",
    "\n",
    "I proceeded to understand the structure of the twitter_enhanced_archive data using .info(). The following issues are visible in from the result above:\n",
    "* tweet ID should be an integer64, not a float\n",
    "* in_reply_to_status_id and in_reply_to_user_id should be an integer64, not float\n",
    "* timestamp should be datetime, no object\n",
    "* there appear to be 181 retweeted items based on retweeted_status_id, retweeted_status_user_id, and retweeted_status_timestamp \n",
    "* retweeted status timestamp should be datetime, not object\n",
    " \n",
    "Checked to see if retweeted_status_timestamp has values populated as this field has the potential to be used to remove retweets. retweeted_status_user_id can be used.  Tested technique to remove the values.  Determined can exclude in_reply_to_status_id and in_reply_to_user_id from the data as \"it is an indicator that the the @username of the sender of the picture is included in the text \n",
    "(source: Tweepy API in the API.update_status section -- http://docs.tweepy.org/en/latest/api.html?highlight=lang#API.search\n",
    "\n",
    "Next, examined the tweets file, obtained from querying the Tweepy API.  tweets also has a field called retweeted_status. We will need to remove records that are retweeted.   I looked at the structure of the tweets dataset using .info().  The following issues were identified\n",
    "* id should be renamed tweet_id\\n\",\n",
    "* in_reply_to_status_id, in_reply_to_status_id_str, in_reply_to_user_id, and in_reply_to_user_id_str are float.\n",
    " * in_reply_to_status_id and in_reply_to_user_id should be integer. in_reply_to_status_id_str and in_reply_to_user_id should be type string\n",
    "* geo appears to have no values\n",
    "* coordinates appers to have no values\n",
    "* place appears to have only 1 value\n",
    "* contributors appears to have no values\n",
    "* there are 163 retweeted_status records. Need to delete them.,\n",
    "* quoted_status_id should be an integer and seems to have very few records (26)\n",
    "* quoted status_id_str should be a string and seems to have very few records (26)\n",
    "*  geo, coordinates, contributors, possibly_sensitive, and possibly sensitive_appealable do not really have any useful data and will be removed.\n",
    "\n",
    "Used value_counts to determine the retweeted field does not provide any useful information as all records are false.  Next, looked at the contents of quoted status.  The quoted_status field does not provide any information that is relevant to this analysis, so, it will be dropped.  Used .describe() to look at the statistics for tweeets file.\n",
    "\n",
    "Explored the text field in further detail.  Changed the column width to be able to view full text.  \n",
    "\n",
    "Looking at the full text, observed the following:\n",
    "* There are tweet users at the beginning of the text field (@serial, @MrRoles, etc.  These could be stripped out into their own column.\n",
    "* Rating is included in text, but could be moved into its own field.  Since it was, it can be removed from this field.\n",
    "* the Twitter url for the tweet is included.  It should be stripped and moved into its own field.\n",
    "\n",
    "Looked at the lang variable using value_counts().  Found the following information in \"lang\" variable.  Spell out the different languages of the tweets. en = English, es = Spanish , tl = Tagalong , ro = Romanian , nl = Nederlands, in = Indonesian, et = Estonian, und = Undetermined, eu = Basque\n",
    "\n",
    "ISO 639-1 codes are used by Tweepy. I found the codes for all of the above except \"und\" which I took to be undetermined. https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes#ET https://www.sitepoint.com/iso-2-letter-language-codes  Looked at each of the codes to determine what language was present in the text. Only one of the non-english texts appear to be non-English. Therefore, we will delete this column as it will not add any value to the analysis.  Based on a review of this column, recommend deleting the \"lang\" variable.  This look like english or jibberish.\n",
    "\n",
    "Next, looked at the truncated field.  Remove the truncated column as the only value is \"False\".\n",
    "\n",
    "Next, looked at the display_text_range field in tweets.  Remove the display_text_range column as it only provides the display text range.   Found it only defines the range in which text is displayed.\n",
    "\n",
    "I then looked at the entities field.  Remove the entities column as it does not provide any data that will be used in the analysis.\n",
    "\n",
    "Looked at the is_quote_status field using value_counts() and .query to view contents.  Determined it is appropriate to drop op is_quote_status as it does not add value to the analysis.\n",
    "\n",
    "Looked at quoted_status_permalink using .sample().  The quoted_status_permalink variable does not appear to add value to the analysis.\n",
    "\n",
    "Next, looked at the image_predictions file.  Based on what we know about the file, the structure seems ok at first glance.  We should look at p1_dog, p2_dog and p3_dog to understand what appears in those variables, using value_counts()Looked at the image_predictions file.  By examining the variables, found the following:\n",
    "* The picture is contained in jpg_url\n",
    "* p1, p2, p3 represent the predicted dog breed. Need to spell out prediction.\n",
    "* p1_conf, p2_conf, p3_conf represent confidence in the prediction. Need to spell out prediction.\n",
    "* Notice that p1, p2 and p3 dog names are lower_case. Need to make first letter uppercase.\n",
    "* Breeds with two words have an underscore. Should be a space.\n",
    "* There are 543 images that are  duplicates in p1\n",
    "* The true and false in these boolean variables indicates if the predicted image is a dog.\n",
    "\n",
    "A description of the image_prediction fields were provided by Udacity at the following link: https://classroom.udacity.com/nanodegrees/nd002/parts/af503f34-9646-4795-a916-190ebc82cb4a/modules/14d9f5f1-9e7b-4bfb-97f3-bcdbf4a3699c/lessons/a8085857-3e28-4fc7-aeb8-da64ccbc2e20/concepts/28d4643b-3785-4700-bdee-4e5fc9963576\n",
    "\n",
    "Next, we need to see if there are any duplicate columns in the datasets.  tweet_id, source, in_reply_to_status_id and in_reply_to_user_id are duplicated.  Since tweet_id = id is common to all files, will need to keep that as the primary key for all datasets.\n",
    "\n",
    "Now, let's look at the image_predictions data structure.  \"Looking at the data, above, we see the following:\n",
    "* The picture is contained in jpg_url\n",
    "* p1, p2, p3 represent the predicted dog breed.  Need to spell out prediction.\n",
    "* p1_conf, p2_conf, p3_conf represent confidence in the prediction.  Need to spell out prediction.\n",
    "* Notice that p1, p2 and p3 dog names are lower_case.  Need to make first letter uppercase.\n",
    "* Breeds with two words have an underscore.  Should be a space.\n",
    "\n",
    "The true and false in these boolean variables indicates if the predicted image is a dog.\n",
    "\n",
    "A description of the image_prediction fields were provided by Udacity at the following link: https://classroom.udacity.com/nanodegrees/nd002/parts/af503f34-9646-4795-a916-190ebc82cb4a/modules/14d9f5f1-9e7b-4bfb-97f3-bcdbf4a3699c/lessons/a8085857-3e28-4fc7-aeb8-da64ccbc2e20/concepts/28d4643b-3785-4700-bdee-4e5fc9963576\n",
    "\n",
    " \n",
    "### Cleaning\n",
    "\n",
    "Before cleaning the data, i made copies of the orignal data files.  I used a combination of programmatic and manual techniques to clean the data, including:\n",
    "* Eliminating retweets\n",
    "* Capitalizing dog names\n",
    "* Stripping URLs from the text field\n",
    "* Changing variable data types\n",
    "* Calculating rating from the rating_numerator and rating_denominator\n",
    "* Removed non-name words from the name field\n",
    "* Deleted unnecessary columns\n",
    "* Eliminated duplicate column names from the twitter_archive_master.csv\n",
    "* Combined multiple dog growth stage columns into a single category\n",
    "\n",
    "After reviewing the text in the Nameless.csv file using Excel, I created another column in the Nameless.csv file where I identified some additional names that were identified in the text string contained in the tweet. Import the revised Nameless.csv file and join the new_name variable to the cleaned_twitter_archive_enhanced dataset.\n",
    "\n",
    "I performed a merge on a list of names that I reviewed and updated in a .csv file.  I then merged all three files together and saved the file as twitter_arhive_master.csv so it was available for use in analysis.\n",
    "\n",
    "Using merge, join the cleaned_twitter_archive_enhanced_v1 and the cleaned_image_predictions files together, with cleaned_twitter_archive_enhanced as the primary and using tweet_id as the primary key.\n",
    "\n",
    "Using merge, join the cleaned_twitter_archive_and_images and the cleaned_tweets files together, with cleaned_twitter_archive_and_images as the primary.\n",
    "\n",
    "### Data Wrangling code is located in wrangle_act.inpyb\n",
    "\n",
    "### Analysis is located in wrangle_report.inpyb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
